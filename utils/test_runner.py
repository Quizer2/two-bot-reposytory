#!/usr/bin/env python3
"""
ðŸ§ª AUTOMATYCZNY RUNNER TESTÃ“W - MODUÅ STARTOWY
== == == == == == == == == == == == == == == == == == == ==

ModuÅ‚ do automatycznego uruchamiania testÃ³w przy starcie aplikacji.
Zapewnia szybkie sprawdzenie stanu systemu przed uruchomieniem gÅ‚Ã³wnej aplikacji.

Autor: CryptoBotDesktop Team
Wersja: 1.0.0
"""

import subprocess
import sys
import os
import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

from utils.logger import get_logger, LogType
from utils.config_manager import get_config_manager, get_app_setting


@dataclass
class TestResult:
    """Wynik pojedynczego testu"""
    __test__ = False
    name: str
    file: str
    status: str  # PASSED, FAILED, ERROR, TIMEOUT, SKIPPED
    duration: float
    error: Optional[str] = None
    output: Optional[str] = None


class StartupTestRunner:
    """
    Runner testÃ³w uruchamianych przy starcie aplikacji
    
    Uruchamia tylko kluczowe testy sprawdzajÄ…ce podstawowÄ… funkcjonalnoÅ›Ä‡
    systemu przed uruchomieniem gÅ‚Ã³wnej aplikacji.
    """
    __test__ = False
    
    def __init__(self):
        self.logger = get_logger("startup_tests")
        self.config_manager = get_config_manager()
        self.project_root = Path(__file__).parent.parent
        self.results: Dict[str, TestResult] = {}
        
        # Konfiguracja testÃ³w startowych
        self.startup_tests_enabled = get_app_setting("startup.run_tests", True)
        self.startup_test_timeout = get_app_setting("startup.test_timeout", 30)
        self.critical_tests_only = get_app_setting("startup.critical_tests_only", True)
        
    def is_enabled(self) -> bool:
        """Sprawdza czy testy startowe sÄ… wÅ‚Ä…czone"""
        return self.startup_tests_enabled
    
    def get_startup_tests(self) -> List[Tuple[str, str, int]]:
        """
        Zwraca listÄ™ testÃ³w do uruchomienia przy starcie
        
        Returns:
            Lista krotek (plik_testu, nazwa_testu, timeout)
        """
        if self.critical_tests_only:
            # Tylko krytyczne testy - szybkie sprawdzenie podstawowej funkcjonalnoÅ›ci
            return [
                ("test_security.py", "Testy BezpieczeÅ„stwa", 20),
                ("test_integration.py", "Testy Integracyjne (podstawowe)", 30),
            ]
        else:
            # PeÅ‚ny zestaw testÃ³w startowych
            return [
                ("test_security.py", "Testy BezpieczeÅ„stwa", 30),
                ("test_integration.py", "Testy Integracyjne", 45),
                ("test_performance.py", "Testy WydajnoÅ›ci (podstawowe)", 30),
            ]
    
    async def run_test_async(self, test_file: str, test_name: str, timeout: int) -> TestResult:
        """
        Uruchamia pojedynczy test asynchronicznie
        
        Args:
            test_file: Nazwa pliku testowego
            test_name: Nazwa testu do wyÅ›wietlenia
            timeout: Timeout w sekundach
            
        Returns:
            Wynik testu
        """
        test_path = self.project_root / test_file
        
        if not test_path.exists():
            self.logger.warning(f"Test file not found: {test_path}")
            return TestResult(
                name=test_name,
                file=test_file,
                status="SKIPPED",
                duration=0.0,
                error=f"Plik testu nie istnieje: {test_file}"
            )
        
        self.logger.info(f"Running startup test: {test_name}")
        start_time = time.time()
        
        try:
            # Przygotuj Å›rodowisko
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.project_root)
            env['TEST_MODE'] = 'startup'
            env['STARTUP_TEST'] = 'true'
            env['PYTHONIOENCODING'] = 'utf-8'  # Napraw problem z kodowaniem Unicode
            
            # Uruchom test
            process = await asyncio.create_subprocess_exec(
                sys.executable, str(test_path),
                cwd=str(self.project_root),
                env=env,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(), 
                    timeout=timeout
                )
                
                duration = time.time() - start_time
                
                if process.returncode == 0:
                    self.logger.info(f"âœ… {test_name}: PASSED ({duration:.1f}s)")
                    return TestResult(
                        name=test_name,
                        file=test_file,
                        status="PASSED",
                        duration=duration,
                        output=stdout.decode('utf-8', errors='ignore')
                    )
                else:
                    error_msg = stderr.decode('utf-8', errors='ignore')
                    self.logger.warning(f"âŒ {test_name}: FAILED ({duration:.1f}s)")
                    return TestResult(
                        name=test_name,
                        file=test_file,
                        status="FAILED",
                        duration=duration,
                        error=error_msg,
                        output=stdout.decode('utf-8', errors='ignore')
                    )
                    
            except asyncio.TimeoutError:
                process.kill()
                await process.wait()
                duration = time.time() - start_time
                self.logger.warning(f"â° {test_name}: TIMEOUT ({timeout}s)")
                return TestResult(
                    name=test_name,
                    file=test_file,
                    status="TIMEOUT",
                    duration=duration,
                    error=f"Test przekroczyÅ‚ limit czasu {timeout}s"
                )
                
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"ðŸ’¥ {test_name}: ERROR - {str(e)}")
            return TestResult(
                name=test_name,
                file=test_file,
                status="ERROR",
                duration=duration,
                error=str(e)
            )
    
    async def run_startup_tests(self, progress_callback=None) -> Tuple[bool, Dict[str, TestResult]]:
        """
        Uruchamia wszystkie testy startowe
        
        Args:
            progress_callback: Funkcja callback do raportowania postÄ™pu
            
        Returns:
            Tuple (success, results)
        """
        if not self.is_enabled():
            self.logger.info("Startup tests disabled in configuration")
            return True, {}
        
        self.logger.info("Starting startup tests...")
        start_time = time.time()
        
        tests = self.get_startup_tests()
        total_tests = len(tests)
        
        if total_tests == 0:
            self.logger.info("No startup tests configured")
            return True, {}
        
        # Uruchom testy
        for i, (test_file, test_name, timeout) in enumerate(tests):
            if progress_callback:
                progress = int((i / total_tests) * 100)
                progress_callback(f"Uruchamianie testu: {test_name}", progress)
            
            result = await self.run_test_async(test_file, test_name, timeout)
            self.results[test_file] = result
            
            # KrÃ³tka przerwa miÄ™dzy testami
            await asyncio.sleep(0.1)
        
        # Analiza wynikÃ³w
        total_duration = time.time() - start_time
        passed_tests = sum(1 for r in self.results.values() if r.status == "PASSED")
        failed_tests = sum(1 for r in self.results.values() if r.status in ["FAILED", "ERROR", "TIMEOUT"])
        skipped_tests = sum(1 for r in self.results.values() if r.status == "SKIPPED")
        
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        overall_success = success_rate >= 70  # 70% testÃ³w musi przejÅ›Ä‡
        
        # Logowanie wynikÃ³w
        self.logger.info(f"Startup tests completed in {total_duration:.1f}s")
        self.logger.info(f"Results: {passed_tests} passed, {failed_tests} failed, {skipped_tests} skipped")
        self.logger.info(f"Success rate: {success_rate:.1f}%")
        
        if overall_success:
            self.logger.info("âœ… Startup tests PASSED - application can start")
        else:
            self.logger.warning("âŒ Startup tests FAILED - application may have issues")
            
            # Loguj szczegÃ³Å‚y bÅ‚Ä™dÃ³w
            for result in self.results.values():
                if result.status in ["FAILED", "ERROR", "TIMEOUT"]:
                    self.logger.error(f"Failed test {result.name}: {result.error}")
        
        return overall_success, self.results
    
    def save_startup_test_results(self, results: Dict[str, TestResult]):
        """Zapisuje wyniki testÃ³w startowych"""
        try:
            logs_dir = self.project_root / "data" / "logs"
            logs_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = logs_dir / f"startup_tests_{timestamp}.json"
            
            # Konwertuj wyniki do formatu JSON
            json_results = {
                "timestamp": timestamp,
                "total_tests": len(results),
                "passed": sum(1 for r in results.values() if r.status == "PASSED"),
                "failed": sum(1 for r in results.values() if r.status in ["FAILED", "ERROR", "TIMEOUT"]),
                "skipped": sum(1 for r in results.values() if r.status == "SKIPPED"),
                "tests": {
                    file: {
                        "name": result.name,
                        "status": result.status,
                        "duration": result.duration,
                        "error": result.error
                    }
                    for file, result in results.items()
                }
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(json_results, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"Startup test results saved to: {results_file}")
            
        except Exception as e:
            self.logger.error(f"Failed to save startup test results: {e}")


def create_startup_test_runner() -> StartupTestRunner:
    """Factory function do tworzenia StartupTestRunner"""
    return StartupTestRunner()